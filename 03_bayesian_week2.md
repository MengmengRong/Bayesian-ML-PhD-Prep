# Week 2: Bayesian Thinking

This document summarizes key concepts from Week 2 of the Bayesian Statistics course on Coursera (UCSC). It covers the basic building blocks of Bayesian inference.

---

## ğŸ”‘ Core Concepts

- Prior distribution
- Likelihood function
- Bayesâ€™ Theorem in action
- Posterior distribution
- Beta and Bernoulli conjugate priors

---

## âœï¸ Notes

> Posterior âˆ Likelihood Ã— Prior  
> \[
P(\theta \mid x) \propto P(x \mid \theta) \cdot P(\theta)
\]

- Rules of Probability

If A and B are two events, the probability that A or B happens (this is an inclusive or, meaning that either A, or B, or both happen) is the probability of the union of the events:

$$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$

where \cup represents union (\or") and \cap represents intersection (\and"). If a set of events A_i for i = 1...,m are mutually exclusive (only one can happen), then

$$ P\left(\bigcup_{i=1}^{m} A_i\right) = \sum_{i=1}^{m} P(A_i) $$ 

- Probabilities can be re-expressed in terms of odds.
- The expected value of a random variable X is a weighted average of values X can take, with weights given by the probabilities of those values.

- Basic Concepts & Notation

  Random Variable (éšæœºå˜é‡)

  X, Y, Z (å¤§å†™å­—æ¯): Denotes a random variable itself.

  x, y, z (å°å†™å­—æ¯): Denotes a specific realization or value of a random variable.

  Distributed As (æœä»...åˆ†å¸ƒ)

  X ~ Distribution(...): Indicates that random variable X follows a specific distribution.

  Example: X ~ Binomial(n, p) (X follows a Binomial distribution with parameters n and p).

- Probability Calculation Formulas
  
  Probability of Union of Events (å¹¶äº‹ä»¶çš„æ¦‚ç‡)

  For any two events A and B (å¯¹äºä»»æ„ä¸¤ä¸ªäº‹ä»¶ A å’Œ B):
  P(A âˆª B) = P(A) + P(B) - P(A âˆ© B)

  For a series of mutually exclusive events Aâ‚, Aâ‚‚, ..., Aâ‚˜ (å¯¹äºä¸€ç³»åˆ—äº’æ–¥äº‹ä»¶ Aâ‚, Aâ‚‚, ..., Aâ‚˜):
  P(â‹ƒáµ¢<binary data, 1 bytes><binary data, 1 bytes>â‚áµ Aáµ¢) = âˆ‘áµ¢<binary data, 1 bytes><binary data, 1 bytes>â‚áµ P(Aáµ¢)

- Definition of Conditional Probability (æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰)

  Definition (å®šä¹‰):
  P(B|A) = P(A âˆ© B) / P(A) (for P(A) > 0)

  Multiplication Rule (ä¹˜æ³•æ³•åˆ™ - derived from definition):
  P(A âˆ© B) = P(B|A)P(A)
  P(A âˆ© B) = P(A|B)P(B)

- Independence of Events (äº‹ä»¶ç‹¬ç«‹æ€§)

  Definition 1 (å®šä¹‰ 1):
  P(A|B) = P(A)

  Definition 2 (Equivalent to Definition 1 - å®šä¹‰ 2ï¼Œç­‰ä»·äºå®šä¹‰ 1):
  P(A âˆ© B) = P(A)P(B)

- Properties of Expectation and Variance

  Expected Value (æœŸæœ›)
  Linearity (çº¿æ€§æ€§è´¨): For any constants a, b and random variables X, Y:
  E(aX + bY) = aE(X) + bE(Y)

  Variance (æ–¹å·®)

  Definition/Calculation Formula (å®šä¹‰/è®¡ç®—å…¬å¼):
  Var(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²

  Variance of Sum (å’Œçš„æ–¹å·®):

  If X and Y are Independent (å¦‚æœ X å’Œ Y ç‹¬ç«‹):
  Var(X + Y) = Var(X) + Var(Y)

  If X and Y are Not Independent (å¦‚æœ X å’Œ Y ä¸ç‹¬ç«‹):
  Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)

  Covariance (åæ–¹å·®)

  Definition (å®šä¹‰):
  Cov(X,Y) = E[(X - E[X])(Y - E[Y])]

  Calculation Formula (è®¡ç®—å…¬å¼ - derived from definition):
  Cov(X,Y) = E[XY] - E[X]E[Y]

---

## ğŸ’¡ Reflections

- How do different priors affect the posterior?
- What happens with more data?
- What applications might benefit from Bayesian thinking?
